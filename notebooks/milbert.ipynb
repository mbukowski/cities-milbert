{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are collecting statistics for following units\n",
    "* level: 6 -> gmina\n",
    "* kind: 1, 4 -> 'miasto powiatowe' or city in 'gmina miejsko-wiejska'\n",
    "* in case a city turned from 'wies' to city in 'gmina miejsko-wiejska' we don't take under consideration statistics befor a 'city' state, it is not that much relevant and additionally it would be most probably treated as a small city - we are only interested in medium size cities\n",
    "* when a city turned from city in 'gmina miejsco-wiejska' to 'miasto powiatowe' kind 1 -> 4 we combine both statistics to keep continouity and remove double stats, the best if mapping is dynamically generated as we may have plenty of cases like these\n",
    "* in case a village turned into a city in current year we won't have statistics for that unit -> we should remove this unit from our research, the same should happen when statistics for a unit do not cover full 5 years, ex. income is only collected for 2 years in 5 years period \n",
    "* in case a city turned into a new administative unit: case of Warszawa or Walbrzych, we need to filter that out manually\n",
    "\n",
    "In order to identify city status we have to group statistics in various categories\n",
    "* population ...\n",
    "\n",
    "We have 2 main periods for research - before 2003 and after -> unemployment rate\n",
    "In order to take under consideration a city to our research it must have values for each parameter\n",
    "\n",
    "for period 2003 earlier we use different mapping of points. If we divide to more groups we could have much more accurate mapping when unemployment rate is not collected.\n",
    "\n",
    "!!! Very important thing. If for any city we don't have statistics collected in each parameter we don't take under consideration this city in our research for given period\n",
    "\n",
    "- überdurchschnittlich wachsend: 19 bis 24 Punkte\n",
    "- wachsend: 14 bis 18 Punkte\n",
    "- keine eindeutige Entwicklungsrichtung: 11 bis 13 Punkte\n",
    "- schrumpfend: 6 bis 10 Punkte\n",
    "- überdurchschnittlich schrumpfend: 0 bis 5 Punkte. \n",
    "\n",
    "TODO \n",
    "Another data cleaninig procedure is to fill gaps for employed and unemlpoyed based on past stats for 'gmina' level \n",
    "and taking under consideration population, working age ratio\n",
    "\n",
    "For unemployed we have to do following\n",
    "* reversed quantile as the lower unemployed rate the better, \n",
    "* collect mean from 2 years to avoid high fluctuations\n",
    "* divide cities into 3 groups not to penalized cities with already low unemployed rate - but based on what?? tbd\n",
    "\n",
    "TODO \n",
    "We may need to update unify unit ids just for gminas as they could have been moved etc. \n",
    "We will use gminas for a data input for employment and unemployed parameters\n",
    "\n",
    "TODO \n",
    "properly define column types that they don't take too much space when calculating\n",
    "\n",
    "Loooks like we will have to do a unifiquation as well for 'gmina' level. as we may have the same problems whenver a gmina moved to a new powiat to keep continuity.\n",
    "\n",
    "TODO \n",
    "When we fill gaps in values for specific units based on parent value and ratio of people in working age to total people in working age in whole parent unit that ratio is much smaller than in reality, that means cities have extra bonus based on something and not only based on working age. If we would use only population that ratio is even smaller\n",
    "\n",
    "TODO \n",
    "Use geometric mean instead of regular mean for all parameters beside migration\n",
    "Migration is still done as a regular mean\n",
    "\n",
    "TODO \n",
    "for unemployment divide cities based on \n",
    "- city type, \n",
    "- unemployment level - but this would have to be averaged for given period and use predefined values 0.1 - 1.0 etc\n",
    "- divide them by the avg of the employment rate -0.5 - +0.5 \n",
    "- use normal distribution \n",
    "\n",
    "TODO \n",
    "Readjustment score based on pre-defined values from Milbert Methodology\n",
    "\n",
    "TODO\n",
    "* 1 - divide unemployment based on specific parameter - so far it's only based on quintile\n",
    "* 2 - extrapolate, approximate data for gaps based on trends of parent unit and not based on ratio of working age in unit vs parent municipality\n",
    "* 3 - polish data with 2 year intervals\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ************************************************** \n",
    "# creates a csv file name with a time linked as a suffix\n",
    "# ************************************************** \n",
    "def csv_name(path:str, name:str, suffix='') -> str:\n",
    "    now = '_' + str(int(time.time()))\n",
    "    return path + '/' + name + suffix + now + '.csv'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ************************************************** \n",
    "# logical units which combine units virtually\n",
    "# ************************************************** \n",
    "logical_units = [\n",
    "    '011212210001', # Miejska strefa usług publicznych\n",
    "    '030210216001', # Miejska strefa usług publicznych\n",
    "    '040416710001', # Miejska strefa usług publicznych\n",
    "    '071412831981', # Związek gmin dzielnic Warszawy do 2001\n",
    "    '071412831991', # GMINY-DZIELNICY WARSZAWY NIE USTALONO do 2001\n",
    "    '020811409102', # Zielona Góra - gmina combined with city\n",
    "    '071412912031'  # Wesoła - city combined with Warszawa and became it's district\n",
    "    ]\n",
    "\n",
    "# ************************************************** \n",
    "# warsaw districts do not bring any added value to analysis, \n",
    "# and they are obsolete after 2001 -> new administration unit is created\n",
    "# ************************************************** \n",
    "warsaw_districts = [\n",
    "    '071412831011', # Warszawa - Bemowo do 2001\n",
    "    '071412831021', # Warszawa - Białołęka do 2001\n",
    "    '071412831031', # Warszawa - Bielany do 2001\n",
    "    '071412831041', # Warszawa - Centrum do 2001\n",
    "    '071412831121', # Warszawa - Rembertów do 2001\n",
    "    '071412831131', # Warszawa - Targówek do 2001\n",
    "    '071412831141', # Warszawa - Ursus do 2001\n",
    "    '071412831151', # Warszawa - Ursynów do 2001\n",
    "    '071412831161', # Warszawa - Wawer do 2001\n",
    "    '071412831171', # Warszawa - Wilanów do 2001\n",
    "    '071412831181'  # Warszawa - Włochy do 2001\n",
    "    ]\n",
    "\n",
    "# ************************************************** \n",
    "# untis with keys should be removed, while data enries shuld be unified to value\n",
    "# ************************************************** \n",
    "merge_dict = { \n",
    "    '011216911021': '011216911024', # Szczawnica\n",
    "    '030210302031': '030210302034', # Pieszyce\n",
    "    '042214011021': '042214011024', # Jastarnia\n",
    "    '042214011041': '042214011044', # Władysławowo\n",
    "    '042214213011': '042214213014', # Czarna Woda\n",
    "    '071412912021': '071412912151', # Sulejówek\n",
    "    '060611211084': '060611211021', # Stoczek Łukowski\n",
    "    '071412831001': '071412865011', # M.st.Warszawa\n",
    "    '030210363011': '030210365011', # Wałbrzych\n",
    "    '030210321091': '030210365011', # Wałbrzych\n",
    "    '011212312021': '012415001081', # Sławków\n",
    "    '020811404094': '020811412014', # Sława\n",
    "    '020811404104': '020811412024', # Szlichtyngowa\n",
    "    '020811404114': '020811412034', # Wschowa\n",
    "    '023216604014': '023216418014', # Dobra\n",
    "    '023216605064': '023216418044', # Resko\n",
    "    '023216614074': '023216418024', # Łobez\n",
    "    '023216614124': '023216418054', # Węgorzyno\n",
    "    '042214209024': '042214216014', # Dzierzgoń\n",
    "    '042214209114': '042214216054', # Sztum\n",
    "    '042815506094': '042815519034', # Węgorzewo\n",
    "    '042815513024': '042815518034', # Gołdap\n",
    "    '051011506011': '051011521011', # Brzeziny\n",
    "    '061813301044': '061813321034'  # Lesko\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ************************************************** \n",
    "# city related functions used mostly in statistics\n",
    "# **************************************************\n",
    "small_city = { 'id': 'small', 'type': 'small', 'min_size': 0, 'max_size': 20000}\n",
    "medium_city = { 'id': 'medium', 'type': 'medium', 'min_size': 20000, 'max_size': 100000}\n",
    "large_city = { 'id': 'large', 'type': 'large', 'min_size': 100000, 'max_size': 100000000}\n",
    "size_margin = 0.1\n",
    "\n",
    "def city_type(size: int) -> int:\n",
    "    medium_min = medium_city['min_size'] - size_margin * medium_city['min_size']\n",
    "    medium_max = medium_city['max_size'] + size_margin * medium_city['max_size']\n",
    "    if size >= medium_min:\n",
    "        if size <= medium_max:\n",
    "            return medium_city['id']\n",
    "        else:\n",
    "            return large_city['id']\n",
    "    else:\n",
    "        return small_city['id']\n",
    "\n",
    "# ************************************************** \n",
    "# city shrinking status\n",
    "#   0-6 -> significantly shrinking\n",
    "#  7-10 -> shrinking\n",
    "# 11-13 -> stablisation\n",
    "# 14-18 -> growing\n",
    "# 19-24 -> significantly growing\n",
    "# **************************************************\n",
    "def city_status(score: int) -> str:\n",
    "    if score <= 6: return 'E'\n",
    "    elif score <= 10: return 'D'\n",
    "    elif score <= 13: return 'C'\n",
    "    elif score <= 18: return 'B'\n",
    "    elif score <= 24: return 'A'\n",
    "    else: return 'F'\n",
    "\n",
    "def city_name(name: str) -> str:\n",
    "    res = name\n",
    "    res = re.sub(' - miasto.*', '', res)\n",
    "    res = re.sub(' od.*', '', res)\n",
    "    res = re.sub(' do.*', '', res)\n",
    "\n",
    "    return res\n",
    "\n",
    "def teryt_id(unit_id: str) -> str:\n",
    "    return unit_id.str[2:4] + unit_id.str[7:12]\n",
    "\n",
    "# ************************************************** \n",
    "# adds a parent id from specific level above to all \n",
    "# 0 -> poland; 1 -> macro-region; 2 -> voivodeship; 3 -> region\n",
    "# 4 -> sub-region; 5 -> county; 6 -> community; 7 -> statistical unit\n",
    "# **************************************************\n",
    "def add_master_id(df, units_df, level):\n",
    "    df['master_id'] = df['parentId']\n",
    "\n",
    "    while True:\n",
    "        df = pd.merge(df, units_df[['id', 'level', 'parentId']], left_on='master_id', right_on='id', suffixes=('', '_drop'))\n",
    "        df['master_id'] = np.where(df['level'] == level, df['master_id'], df['parentId_drop'])\n",
    "        df.drop([col for col in df.columns if 'drop' in col], axis=1, inplace=True)\n",
    "\n",
    "        if df.loc[df.level != level, 'level'].count() == 0:\n",
    "            df.drop(['level'], axis=1, inplace=True)\n",
    "            break\n",
    "\n",
    "        df.drop(['level'], axis=1, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ************************************************** \n",
    "# statistic periods\n",
    "# **************************************************\n",
    "sort_order = ['id', 'year']\n",
    "periods = [1996, 2001, 2006, 2011, 2016]\n",
    "# periods = [2006, 2011, 2016]\n",
    "period_length = 5\n",
    "\n",
    "# ************************************************** \n",
    "# quantile distribution\n",
    "# **************************************************\n",
    "quantile_distribution = [.2, .4, .6, .8]\n",
    "\n",
    "# v is value, and q is list of quantiles ordered \n",
    "def quantile_score(v: float, q: list[float], reversed=False) -> int:\n",
    "    res = len(q)\n",
    "\n",
    "    for i in range(len(q)):\n",
    "        if v < q[i]:\n",
    "            res = i\n",
    "            break\n",
    "\n",
    "    if reversed:\n",
    "        res = len(q) - res\n",
    "\n",
    "    return res\n",
    "\n",
    "# ************************************************** \n",
    "# unify unit id to a single one\n",
    "# **************************************************\n",
    "def unify_id(id: str) -> str:\n",
    "    if id in merge_dict:\n",
    "        return merge_dict[id]\n",
    "    \n",
    "    return id\n",
    "\n",
    "# ************************************************** \n",
    "# counts parameter ratio per 1000 population\n",
    "# **************************************************\n",
    "def param_ratio_per_1000(population, param) -> float:\n",
    "    ratio = 0.001 * population\n",
    "    result = param / ratio\n",
    "\n",
    "    return result\n",
    "\n",
    "# ************************************************** \n",
    "# counts ratio between two parameters\n",
    "# **************************************************\n",
    "def param_ratio(param1, param2) -> float:\n",
    "    return param1 / param2\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ************************************************** \n",
    "# loads a main df from the csv file and renames a value column\n",
    "# **************************************************\n",
    "def init_main_df(path, var_id, param):\n",
    "    df = pd.read_csv(path, dtype={'unitId': str})\n",
    "    df = df[df['varId'] == var_id].copy()\n",
    "    df['unitId'] = df.apply(lambda row: unify_id(row['unitId']), axis=1)\n",
    "    df.rename(columns={'val': param, 'unitId': 'id'}, inplace=True)\n",
    "    df.drop(['varId'], axis=1, inplace=True)\n",
    "    df.sort_values(sort_order, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# ************************************************** \n",
    "# craetes a step df which is the merge between cities - left \n",
    "# and main - right; which is specific df for given step\n",
    "# **************************************************\n",
    "def init_step_df(cities_df, main_df):\n",
    "    df = pd.merge(cities_df, main_df, on='id')\n",
    "    df.sort_values(sort_order, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# ************************************************** \n",
    "# craetes a step df which is the merge between cities - left \n",
    "# and main - right; which is specific df for given step\n",
    "# extended version when we have to add ratio parameters to value\n",
    "# **************************************************\n",
    "def init_step_df_ext(cities_df, main_df, param, ratio_df, ratio):\n",
    "    ratio_param = '_'.join([ratio, param, 'drop'])\n",
    "    rate_ratio = '_'.join(['rate', ratio, 'drop'])\n",
    "    parent_param = '_'.join(['parent', param, 'drop'])\n",
    "    unit_param = '_'.join(['unit', param, 'drop'])\n",
    "    \n",
    "    step_df = init_step_df(cities_df, main_df)\n",
    "    step_df = enrich_with_parent(step_df, main_df, param) # adds data for parent\n",
    "    step_df = enrich_with_ratio(step_df, ratio_df, ratio) # adds ratio rate\n",
    "    \n",
    "    step_df[ratio_param] = step_df[rate_ratio] * step_df[parent_param]\n",
    "    step_df[param] = np.where(step_df[unit_param] == 0, step_df[ratio_param], step_df[unit_param])\n",
    "    \n",
    "    return step_df\n",
    "\n",
    "\n",
    "# ************************************************** \n",
    "# enriches a step df with parent data\n",
    "# used for employment, unemployed and revenue df\n",
    "# **************************************************\n",
    "def enrich_with_parent(step_df, main_df, param):\n",
    "    drop_param = '_'.join([param, 'drop'])\n",
    "    parent_param = '_'.join(['parent', param, 'drop'])\n",
    "    unit_param = '_'.join(['unit', param, 'drop'])\n",
    "    \n",
    "    step_df = pd.merge(step_df, main_df[['id', 'year', param]], left_on=['parentId', 'year'], right_on=['id', 'year'], suffixes=('', '_drop'))\n",
    "    step_df.rename(columns={param: unit_param, drop_param: parent_param}, inplace=True)\n",
    "    step_df.drop(['id_drop'], axis=1, inplace=True)\n",
    "\n",
    "    return step_df\n",
    "\n",
    "\n",
    "# ************************************************** \n",
    "# enriches a step df with ratio which can be used to fill statistics gaps - applied to parent data\n",
    "# used for employment, unemployed and revenue df\n",
    "# **************************************************\n",
    "def enrich_with_ratio(step_df, ratio_df, ratio):\n",
    "    drop_ratio = '_'.join([ratio, 'drop'])\n",
    "    parent_ratio = '_'.join(['parent', ratio, 'drop'])\n",
    "    rate_ratio = '_'.join(['rate', ratio, 'drop'])\n",
    "\n",
    "    step_df = pd.merge(step_df, ratio_df[['id', 'year', ratio]], on=['id', 'year'])\n",
    "    step_df = pd.merge(step_df, ratio_df[['id', 'year', ratio]], left_on=['parentId', 'year'], right_on=['id', 'year'], suffixes=('', '_drop'))\n",
    "    step_df.rename(columns={drop_ratio: parent_ratio}, inplace=True)\n",
    "    step_df.drop(['id_drop'], axis=1, inplace=True)\n",
    "    step_df[rate_ratio] = step_df[ratio] / step_df[parent_ratio]\n",
    "\n",
    "    return step_df\n",
    "\n",
    "\n",
    "# ************************************************** \n",
    "# drops from the df all columns with 'drop' suffix\n",
    "# sorts and reindexes\n",
    "# **************************************************\n",
    "def cleanup_df(df):\n",
    "    df.drop([col for col in df.columns if 'drop' in col], axis=1, inplace=True)\n",
    "    df.sort_values(sort_order, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "# ************************************************** \n",
    "# craetes a stats df based on step df\n",
    "# we filter out data for the end of given periods\n",
    "# we drop param column as we are not interested in value for current year\n",
    "# we are more interested in aggregation in the 5 year span \n",
    "# **************************************************\n",
    "def init_stats_df(step_df, drop_list):\n",
    "    df = step_df[step_df['year'].isin([x + period_length for x in periods])].copy()\n",
    "    df = df.dropna()\n",
    "    df.drop(drop_list, axis=1, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ************************************************** \n",
    "# calculates a quantile score for given stats\n",
    "# **************************************************\n",
    "def stats_quantile_score(stats_df, score, reversed=False):\n",
    "    stats_df[score] = np.NaN\n",
    "\n",
    "    by_year = stats_df.groupby('year')\n",
    "    for year, frame in by_year:\n",
    "        quantiles = frame['mean'].quantile(quantile_distribution).values.tolist()\n",
    "        frame[score] = frame.apply(lambda row: quantile_score(row['mean'], quantiles, reversed=reversed), axis=1)\n",
    "        stats_df.update(frame)\n",
    "\n",
    "# ************************************************** \n",
    "# appends stats dataframe to existing one for given param\n",
    "# **************************************************\n",
    "def append_stats(stats_df, append_df, param): \n",
    "    param_score = '_'.join([param, 'score'])\n",
    "    param_mean = '_'.join([param, 'mean'])\n",
    "    \n",
    "    df = pd.merge(stats_df, append_df[['id', 'year', 'mean', param_score]], left_on=['id', 'year'], right_on=['id', 'year'])\n",
    "    df.rename(columns={'mean': param_mean}, inplace=True)\n",
    "\n",
    "    df['score'] = df['score'] + df[param_score]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "units_df = pd.read_csv('../data/bdl/units.csv', dtype={'id': str, 'parentId': str})\n",
    "population_df = init_main_df('../data/bdl/data_unit_population_short.csv', 72305, 'population')\n",
    "migration_df = init_main_df('../data/bdl/data_unit_migration_outer_short.csv', 1365234, 'migration')\n",
    "working_age_df = init_main_df('../data/bdl/data_unit_working_age_short.csv', 152, 'working_age')\n",
    "employment_df = init_main_df('../data/bdl/data_unit_employment_short.csv', 54821, 'employment')\n",
    "unemployed_df = init_main_df('../data/bdl/data_unit_registered_unemployed_short.csv', 10514, 'unemployed')\n",
    "own_revenue_df = init_main_df('../data/bdl/data_unit_own_revenue_short.csv', 76070, 'own_revenue')\n",
    "\n",
    "units_gcs_df = pd.read_csv('../data/arcgis/units_gcs.csv', dtype={'id': str})\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_lvl = [6]\n",
    "city_types = [1, 4]\n",
    "\n",
    "cities_df = units_df[(units_df['level'].isin(community_lvl)) & units_df['kind'].isin(city_types)].copy()\n",
    "\n",
    "cities_df = cities_df[~cities_df['id'].isin(logical_units)]\n",
    "cities_df = cities_df[~cities_df['id'].isin(warsaw_districts)]\n",
    "cities_df = cities_df[~cities_df['id'].isin(merge_dict.keys())]\n",
    "\n",
    "# below could be implemented as lambda but we would have to move column to position 1\n",
    "teryt_id = cities_df['id'].str[2:4] + cities_df['id'].str[7:12]\n",
    "cities_df.insert(1, 'teryt_id', teryt_id)\n",
    "\n",
    "cities_df['name'] = cities_df.apply(lambda row: city_name(row['name']), axis=1)\n",
    "\n",
    "# clean up\n",
    "cities_df.drop(['level', 'kind', 'hasDescription', 'description', 'years'], axis=1, inplace=True)\n",
    "cities_df.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = 'population'\n",
    "score = '_'.join([param, 'score'])\n",
    "\n",
    "step_df = init_step_df(cities_df, population_df)\n",
    "\n",
    "step_df['diff'], step_df['rate'], step_df['mean'] = np.NaN, np.NaN, np.NaN\n",
    "by_id = step_df.groupby('id')\n",
    "for id, frame in by_id:\n",
    "    frame['diff'] = frame[param].diff()\n",
    "    frame['rate'] = frame['diff'] / (frame[param] - frame['diff'])\n",
    "    frame['mean'] = frame['rate'].rolling(window=5).mean()\n",
    "    step_df.update(frame)\n",
    "\n",
    "stats_df = init_stats_df(step_df, [param, 'parentId', 'diff', 'rate'])\n",
    "stats_quantile_score(stats_df, score)\n",
    "\n",
    "population_step_df = step_df\n",
    "population_step_df.to_csv(csv_name('../figures', 'step_population'))\n",
    "population_stats_df = stats_df\n",
    "population_stats_df.to_csv(csv_name('../figures', 'stats_population'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Migration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = 'migration'\n",
    "score = '_'.join([param, 'score'])\n",
    "\n",
    "step_df = init_step_df(cities_df, migration_df)\n",
    "\n",
    "step_df = pd.merge(step_df, population_df[['id', 'year', 'population']], on=['id', 'year'])\n",
    "step_df['rate'] = step_df['migration'] / (0.001 * step_df['population'])\n",
    "\n",
    "step_df['mean'] = np.NaN\n",
    "by_id = step_df.groupby('id')\n",
    "for id, frame in by_id:\n",
    "    frame['mean'] = frame['rate'].rolling(window=5).mean()\n",
    "    step_df.update(frame)\n",
    "\n",
    "stats_df = init_stats_df(step_df, [param, 'parentId', 'population', 'rate'])\n",
    "stats_quantile_score(stats_df, score)\n",
    "\n",
    "migration_step_df = step_df\n",
    "migration_step_df.to_csv(csv_name('../figures', 'step_migration'))\n",
    "migration_stats_df = stats_df\n",
    "migration_stats_df.to_csv(csv_name('../figures', 'stats_migration'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = 'working_age'\n",
    "score = '_'.join([param, 'score'])\n",
    "\n",
    "step_df = init_step_df(cities_df, working_age_df)\n",
    "\n",
    "step_df['diff'], step_df['rate'], step_df['mean'] = np.NaN, np.NaN, np.NaN\n",
    "by_id = step_df.groupby('id')\n",
    "for id, frame in by_id:\n",
    "    frame['diff'] = frame[param].diff()\n",
    "    frame['rate'] = frame['diff'] / (frame[param] - frame['diff'])\n",
    "    frame['mean'] = frame['rate'].rolling(window=5).mean()\n",
    "    step_df.update(frame)\n",
    "\n",
    "stats_df = init_stats_df(step_df, [param, 'parentId', 'diff', 'rate'])\n",
    "stats_quantile_score(stats_df, score)\n",
    "\n",
    "working_age_step_df = step_df\n",
    "working_age_step_df.to_csv(csv_name('../figures', 'step_working_age'))\n",
    "working_age_stats_df = stats_df\n",
    "working_age_stats_df.to_csv(csv_name('../figures', 'stats_working_age'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Employment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = 'employment'\n",
    "score = '_'.join([param, 'score'])\n",
    "ratio = 'working_age'\n",
    "\n",
    "step_df = init_step_df_ext(cities_df, employment_df, param, working_age_df, ratio)\n",
    "cleanup_df(step_df)\n",
    "step_df = step_df.astype({param: int})\n",
    "\n",
    "step_df['diff'], step_df['rate'], step_df['mean'] = np.NaN, np.NaN, np.NaN\n",
    "by_id = step_df.groupby('id')\n",
    "for id, frame in by_id:\n",
    "    frame['diff'] = frame[param].diff()\n",
    "    frame['rate'] = frame['diff'] / (frame[param] - frame['diff'])\n",
    "    frame['mean'] = frame['rate'].rolling(window=5).mean()\n",
    "    step_df.update(frame)\n",
    "\n",
    "stats_df = init_stats_df(step_df, [param, ratio, 'parentId', 'diff', 'rate'])\n",
    "stats_quantile_score(stats_df, score)\n",
    "\n",
    "employment_step_df = step_df\n",
    "employment_step_df.to_csv(csv_name('../figures', 'step_employment'))\n",
    "employment_stats_df = stats_df\n",
    "employment_stats_df.to_csv(csv_name('../figures', 'stats_employment'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registered Unemployed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = 'unemployed'\n",
    "rate = '_'.join([param, 'rate'])\n",
    "score = '_'.join([param, 'score'])\n",
    "ratio = 'working_age'\n",
    "\n",
    "step_df = init_step_df_ext(cities_df, unemployed_df, param, working_age_df, ratio)\n",
    "cleanup_df(step_df)\n",
    "step_df = step_df.astype({param: int})\n",
    "\n",
    "# first we need to calculate a unemployed_rate for that we need value of working_age population\n",
    "# it just happened that working age and ratio are the same\n",
    "# we count unemployed rate in % points, that's why we don't have ratio year by year\n",
    "step_df[rate], step_df['diff'], step_df['mean'] = np.NaN, np.NaN, np.NaN\n",
    "by_id = step_df.groupby('id')\n",
    "for id, frame in by_id:\n",
    "    frame[rate] = frame[param] / frame[ratio]\n",
    "    frame['diff'] = frame[rate].diff()\n",
    "    frame['mean'] = frame['diff'].rolling(window=5).mean()\n",
    "    step_df.update(frame)\n",
    "\n",
    "stats_df = init_stats_df(step_df, [param, ratio, 'parentId', 'diff'])\n",
    "stats_quantile_score(stats_df, score, reversed=True)\n",
    "\n",
    "unemployed_step_df = step_df\n",
    "unemployed_step_df.to_csv(csv_name('../figures', 'step_unemployed'))\n",
    "unemployed_stats_df = stats_df\n",
    "unemployed_stats_df.to_csv(csv_name('../figures', 'stats_unemployed'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Own Revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = 'own_revenue'\n",
    "param_pp = '_'.join([param, 'pp'])\n",
    "score = '_'.join([param, 'score'])\n",
    "ratio = 'working_age'\n",
    "\n",
    "step_df = init_step_df_ext(cities_df, own_revenue_df, param, working_age_df, ratio)\n",
    "cleanup_df(step_df)\n",
    "\n",
    "step_df = pd.merge(step_df, population_df[['id', 'year', 'population']], on=['id', 'year'])\n",
    "step_df[param_pp] = step_df['own_revenue'] / step_df['population']\n",
    "\n",
    "step_df['diff'], step_df['rate'], step_df['mean'] = np.NaN, np.NaN, np.NaN\n",
    "by_id = step_df.groupby('id')\n",
    "for id, frame in by_id:\n",
    "    frame['diff'] = frame['own_revenue_pp'].diff()\n",
    "    frame['rate'] = frame['diff'] / (frame[param_pp] - frame['diff'])\n",
    "    frame['mean'] = frame['rate'].rolling(window=5).mean()\n",
    "    step_df.update(frame)\n",
    "\n",
    "stats_df = init_stats_df(step_df, [param, param_pp, ratio, 'parentId', 'population', 'diff', 'rate'])\n",
    "stats_quantile_score(stats_df, score)\n",
    "\n",
    "own_revenue_step_df = step_df\n",
    "own_revenue_step_df.to_csv(csv_name('../figures', 'step_own_revenue'))\n",
    "own_revenue_stats_df = stats_df\n",
    "own_revenue_stats_df.to_csv(csv_name('../figures', 'stats_own_revenue'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df = population_stats_df[['id', 'teryt_id', 'name', 'year']].copy()\n",
    "stats_df = pd.merge(stats_df, population_df[['id', 'year', 'population']], left_on=['id', 'year'], right_on=['id', 'year'])\n",
    "\n",
    "# applying city type could be also done in a dashboard esp when we have a ratio parameter included\n",
    "stats_df['city_type'] = stats_df.apply(lambda row: city_type(row['population']), axis=1)\n",
    "stats_df = pd.merge(stats_df, units_gcs_df[['id', 'latitude', 'longitude']], on=['id'])\n",
    "stats_df['score'] = 0\n",
    "stats_df['status'] = np.NaN\n",
    "\n",
    "stats_df = append_stats(stats_df, population_stats_df, 'population')\n",
    "stats_df = append_stats(stats_df, migration_stats_df, 'migration')\n",
    "stats_df = append_stats(stats_df, working_age_stats_df, 'working_age')\n",
    "stats_df = append_stats(stats_df, employment_stats_df, 'employment')\n",
    "stats_df = append_stats(stats_df, unemployed_stats_df, 'unemployed')\n",
    "stats_df = append_stats(stats_df, own_revenue_stats_df, 'own_revenue')\n",
    "\n",
    "stats_df['status'] = stats_df.apply(lambda row: city_status(row['score']), axis=1)\n",
    "\n",
    "by_year = stats_df.groupby('year')\n",
    "for year, frame in by_year:    \n",
    "    name = '_'.join([str(int(year)), 'shrinking_cities'])\n",
    "    frame.to_csv(csv_name('../figures', name))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below code is an example how to find list of unit ids which require to be unified\n",
    "\n",
    "# df = units_df[units_df['description'].str.len() > 0] # 675\n",
    "# df = df[~df['description'].str.startswith('Zmiana rodzaju gminy z wiejskiego na miejsko-wiejski')] # 675 - 406 = 269\n",
    "# df = df[~df['description'].str.startswith('Zmiana granic')] # 269 - 25 = 244\n",
    "# df = df[~df['description'].str.startswith('Utworzenie')] # 244 - 32 = 212\n",
    "# df = df[~df['description'].str.startswith('Zmiana przynależności')] # 212 - 114 = 98 ***\n",
    "# df = df[~df['description'].str.startswith('Zmiana rodzaju gminy z miejskiego na miejsko-wiejski')] # 98 - 20 = 78 ***\n",
    "# df = df[~df['description'].str.startswith('Przeniesienie gminy')] # 78 - 4 = 74 ***\n",
    "# df = df[~df['description'].str.startswith('Zmiana symbolu gminy')] # 74 - 2 = 72 ***\n",
    "# df = df[~df['description'].str.startswith('Zmiana rodzaju gminy')] # 72 - 6 = 66 *** / leftovers \n",
    "# df = df[~df['description'].str.startswith('Zmiana ustroju miasta')] # 66 - 2 = 64 ***\n",
    "# df = df[~df['description'].str.startswith('Przywrócenie miastu Wałbrzych statusu')] # 64 - 2 = 62 ***\n",
    "# df = df[~df['description'].str.startswith('Zniesienia gminy')] # 62 - 1 = 61\n",
    "# df = df[~df['description'].str.startswith('Połączenie miasta')] # 61 - 1 = 60\n",
    "# df = df[~df['description'].str.startswith('Zniesienie')] # 60 - 18 = 42 ***\n",
    "# df = df[~df['description'].str.startswith('Zmiana nazwy')] # 42"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
